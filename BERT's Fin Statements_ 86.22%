{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"provenance":[{"file_id":"1PVYvTW0kAkkQEII22RfX97rvLe0YG80Z","timestamp":1725375479244},{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/bert-s-a-stock-market-guru-86-22-huggingface-1c88ce11-27ff-4fa8-8526-e21895ab46ae.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20240831/auto/storage/goog4_request&X-Goog-Date=20240831T181927Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=5346e1d0be583118078615d75d333620ff97c49d9cda9560d2e37b9c07e48b55f8dbbdee36ee24782d2bc3343d5907ce027e050d39c8b6e0541caeed44cfa75cf703939ba15bb52a1127da829cef13ce44a680b29ee657e8d1012cbfaad410f8b3659279d01cb0d060460c2a07b51417dd1d255b47f8fba0b3e723d32160594d4b4a0771142a149bb5ba38dfd41bd92035f5d6f7a052acab08adae8837b99425a2b731510364e856b13ba06dcf05f1cdcf8e28f4ea44391f0b97acbb6c0e07c0266939cf1858542fcc1d48314b3fdd803a745b4b31847f9a9d424bdabcaa378132bd9f36c045b317816406d511f401f4495b5b522fc476b426b4800bd6402c38","timestamp":1725128444191}]}},"nbformat_minor":0,"nbformat":4,"cells":[{"source":["import os\n","import sys\n","from tempfile import NamedTemporaryFile\n","from urllib.request import urlopen\n","from urllib.parse import unquote, urlparse\n","from urllib.error import HTTPError\n","from zipfile import ZipFile\n","import tarfile\n","import shutil\n","\n","CHUNK_SIZE = 40960\n","DATA_SOURCE_MAPPING = 'financial-sentiment-analysis:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1918992%2F3205803%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240831%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240831T181927Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dab0b5030cb7aa324759f57179e5c419606807936e3e6c61b4f5b14d64a5724791bbe19f21b74341386292f95f3d3b422194c1052aa593a2e5f59fb8d401f5d2753d9e8f898f861af5108472a689facb037b3e3328eeef1e67584eb9354a083b6a816c8ea2f0bc6cd8bf6267045955809e3dc4657a548bbac9ccb353580b14c1985c26867519770f3002392fe751e16fe18b0f1898681cec5b9aafada94f5b79857c17ed15c69153e1d1d7afc5a08f3707811e277d9edf4a4fff6fb64a2ba3a6111b5223040ba28c3663ce25fbcfca35ecbada79a03ce58f60e3c40cc566e34a96ddf7d09e2c9e22f0b5dcd2a192a4de550fd579a790b05af0c05d0d958aa6c00'\n","\n","KAGGLE_INPUT_PATH='/kaggle/input'\n","KAGGLE_WORKING_PATH='/kaggle/working'\n","KAGGLE_SYMLINK='kaggle'\n","\n","!umount /kaggle/input/ 2> /dev/null\n","shutil.rmtree('/kaggle/input', ignore_errors=True)\n","os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n","os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n","\n","try:\n","  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","try:\n","  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","\n","for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n","    directory, download_url_encoded = data_source_mapping.split(':')\n","    download_url = unquote(download_url_encoded)\n","    filename = urlparse(download_url).path\n","    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n","    try:\n","        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n","            total_length = fileres.headers['content-length']\n","            print(f'Downloading {directory}, {total_length} bytes compressed')\n","            dl = 0\n","            data = fileres.read(CHUNK_SIZE)\n","            while len(data) > 0:\n","                dl += len(data)\n","                tfile.write(data)\n","                done = int(50 * dl / int(total_length))\n","                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n","                sys.stdout.flush()\n","                data = fileres.read(CHUNK_SIZE)\n","            if filename.endswith('.zip'):\n","              with ZipFile(tfile) as zfile:\n","                zfile.extractall(destination_path)\n","            else:\n","              with tarfile.open(tfile.name) as tarfile:\n","                tarfile.extractall(destination_path)\n","            print(f'\\nDownloaded and uncompressed: {directory}')\n","    except HTTPError as e:\n","        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n","        continue\n","    except OSError as e:\n","        print(f'Failed to load {download_url} to path {destination_path}')\n","        continue\n","\n","print('Data source import complete.')\n"],"metadata":{"id":"YURYWUsRqd_A"},"cell_type":"code","outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"LBIVyiBYqd_L"}},{"cell_type":"code","source":["%%capture\n","!pip install transformers\n","!pip install accelerate\n","# uncomment below cell to train on TPU's\n","#!pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n","!pip install git+https://github.com/huggingface/accelerate\n","!pip install ml_collections\n","!pip install datasets\n","!pip install pandas-profiling[notebook]"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-27T19:04:42.912027Z","iopub.execute_input":"2022-02-27T19:04:42.912561Z","iopub.status.idle":"2022-02-27T19:05:39.458423Z","shell.execute_reply.started":"2022-02-27T19:04:42.912474Z","shell.execute_reply":"2022-02-27T19:05:39.457468Z"},"trusted":true,"id":"SL6HUeASqd_M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import ml_collections\n","import datasets\n","import torch\n","import transformers\n","from accelerate import Accelerator, DistributedType\n","from datasets import load_metric, Dataset, DatasetDict\n","from torch.utils.data import DataLoader\n","from tqdm.auto import tqdm\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    get_linear_schedule_with_warmup,\n","    set_seed,\n",")\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, precision_score\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","\n","import emoji\n","from wordcloud import WordCloud, STOPWORDS\n","import re,string, nltk\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk import word_tokenize\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from nltk.stem.snowball import SnowballStemmer\n","\n","from pandas_profiling import ProfileReport\n","\n","import warnings\n","warnings.filterwarnings(action=\"ignore\")"],"metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:05:39.460442Z","iopub.execute_input":"2022-02-27T19:05:39.460723Z","iopub.status.idle":"2022-02-27T19:05:53.274369Z","shell.execute_reply.started":"2022-02-27T19:05:39.460685Z","shell.execute_reply":"2022-02-27T19:05:53.273545Z"},"trusted":true,"id":"F1sy7uBAqd_N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setting up the model hyperparameters\n","\n","def model_config():\n","    cfg_dictionary = {\n","        \"data_path\": \"../input/financial-sentiment-analysis/data.csv\",\n","        \"model_path\": \"/kaggle/working/bert_model.h5\",\n","        \"model_type\": \"transformer\",\n","\n","        \"test_size\": 0.1,\n","        \"validation_size\":0.2,\n","        \"train_batch_size\": 32,\n","        \"eval_batch_size\": 32,\n","\n","        \"epochs\": 5,\n","        \"adam_epsilon\": 1e-8,\n","        \"lr\": 3e-5,\n","        \"num_warmup_steps\": 10,\n","\n","        \"max_length\": 128,\n","        \"random_seed\": 42,\n","        \"num_labels\": 3,\n","        \"model_checkpoint\":\"roberta-base\",\n","    }\n","    cfg = ml_collections.FrozenConfigDict(cfg_dictionary)\n","\n","    return cfg\n","cfg = model_config()"],"metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:05:53.275726Z","iopub.execute_input":"2022-02-27T19:05:53.277293Z","iopub.status.idle":"2022-02-27T19:05:53.288725Z","shell.execute_reply.started":"2022-02-27T19:05:53.277252Z","shell.execute_reply":"2022-02-27T19:05:53.287959Z"},"trusted":true,"id":"V14p72yZqd_O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocessing"],"metadata":{"id":"ZxICVSIrqd_P"}},{"cell_type":"markdown","source":["Almost all text we find is unsanitised and contains many characters that can trip up out model, we perform a regime of cleaning steps that remove URL's, Emoji's, Special Characters that appear very infrequenty in the english language. When it comes to applying non deep learning models, such as XGBoost, Decision Trees etc, we perfom stemming and lemmatization that further simplifies the word in the text. For, Large Language Models (LLM's) these preporcessing steps seem unnessary, and so we skip them."],"metadata":{"id":"W4RD3OjAqd_Q"}},{"cell_type":"code","source":["def clean_text(df,field):\n","    df[field] = df[field].str.replace(r\"http\\S+\",\" \")\n","    df[field] = df[field].str.replace(r\"http\",\" \")\n","    df[field] = df[field].str.replace(r\"@\",\"at\")\n","    df[field] = df[field].str.replace(\"#[A-Za-z0-9_]+\", ' ')\n","    df[field] = df[field].str.replace(r\"[^A-Za-z(),!?@\\'\\\"_\\n]\",\" \")\n","    df[field] = df[field].str.lower()\n","    return df\n","\n","lemmatizer = WordNetLemmatizer()\n","stemmer = SnowballStemmer(\"english\")\n","STOPWORDS.update(['rt', 'mkr', 'didn', 'bc', 'n', 'm','im', 'll', 'y', 've',\n","                      'u', 'ur', 'don','p', 't', 's', 'aren', 'kp', 'o', 'kat',\n","                      'de', 're', 'amp', 'will'])\n","\n","def preprocess_text(text):\n","    text = re.sub(r\"won\\'t\", \"will not\", text)\n","    text = re.sub(r\"can\\'t\", \"can not\", text)\n","    text = re.sub(r\"n\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'re\", \" are\", text)\n","    text = re.sub(r\"\\'s\", \" is\", text)\n","    text = re.sub(r\"\\'d\", \" would\",text)\n","    text = re.sub(r\"\\'ll\", \" will\", text)\n","    text = re.sub(r\"\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'ve\", \" have\", text)\n","    text = re.sub(r\"\\'m\", \" am\", text)\n","    text = re.sub('[^a-zA-Z]',' ',text)\n","    text = re.sub(emoji.get_emoji_regexp(),\"\",text)\n","    text = re.sub(r'[^\\x00-\\x7f]','',text)\n","    text = \" \".join([stemmer.stem(word) for word in text.split()])\n","    text = [lemmatizer.lemmatize(word) for word in text.split() if not word in set(STOPWORDS)]\n","    text = ' '.join(text)\n","    return text\n"],"metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:05:53.292123Z","iopub.execute_input":"2022-02-27T19:05:53.292537Z","iopub.status.idle":"2022-02-27T19:05:53.306992Z","shell.execute_reply.started":"2022-02-27T19:05:53.292507Z","shell.execute_reply":"2022-02-27T19:05:53.306143Z"},"trusted":true,"id":"SfBsBt-bqd_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_csv(csv_file: str) -> pd.DataFrame:\n","    df = pd.read_csv(csv_file)\n","\n","    labelencoder = LabelEncoder()\n","    df[\"label_enc\"] = labelencoder.fit_transform(df[\"Sentiment\"])\n","    df.rename(columns={\"label\": \"label_desc\"}, inplace=True)\n","    df.rename(columns={\"label_enc\": \"labels\"}, inplace=True)\n","    df.drop_duplicates(subset=['Sentence'],keep='first',inplace=True)\n","\n","    cleaned_df = clean_text(df, \"Sentence\")\n","\n","    if cfg.model_type is not \"transformer\":\n","        cleaned_df[\"Sentence\"] = cleaned_df[\"Sentence\"].apply(preprocess_text)\n","    return cleaned_df"],"metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:05:53.308196Z","iopub.execute_input":"2022-02-27T19:05:53.308472Z","iopub.status.idle":"2022-02-27T19:05:53.318528Z","shell.execute_reply.started":"2022-02-27T19:05:53.308438Z","shell.execute_reply":"2022-02-27T19:05:53.31787Z"},"trusted":true,"id":"pSkD6vX4qd_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## EDA\n","\n","Pandas Profiling is an amazing library that helps you extract important information from dataframes in one shot. No need to call `df.head()`. `df.describe()`, `df.info()` multiple times to perform EDA, Pandas Profiling takes care of that and much more for you."],"metadata":{"id":"ae7GnMzsqd_S"}},{"cell_type":"code","source":["df = preprocess_csv(cfg.data_path)\n","profile = ProfileReport(df, title=\"Financial Sentiment Analysis\")\n","profile.to_notebook_iframe()"],"metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:05:53.321407Z","iopub.execute_input":"2022-02-27T19:05:53.32161Z","iopub.status.idle":"2022-02-27T19:06:02.043117Z","shell.execute_reply.started":"2022-02-27T19:05:53.321582Z","shell.execute_reply":"2022-02-27T19:06:02.042281Z"},"trusted":true,"id":"aPuqAvoxqd_S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ML Based Approach"],"metadata":{"id":"Jfr-drGHqd_S"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(np.array(df[\"Sentence\"]),np.array(df[\"labels\"]), test_size=0.25, random_state=42)"],"metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:06:02.044065Z","iopub.execute_input":"2022-02-27T19:06:02.044284Z","iopub.status.idle":"2022-02-27T19:06:02.053298Z","shell.execute_reply.started":"2022-02-27T19:06:02.044257Z","shell.execute_reply":"2022-02-27T19:06:02.052545Z"},"trusted":true,"id":"Wt_IQz5tqd_T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tfidf = TfidfVectorizer(use_idf=True, tokenizer=word_tokenize,min_df=0.00002,max_df=0.70)\n","X_train_tf = tfidf.fit_transform(X_train.astype('U'))\n","X_test_tf = tfidf.transform(X_test.astype('U'))\n","\n","print(f\"TF_IDF Model: Train features shape:{X_train_tf.shape} and Test features shape:{X_test_tf.shape}\")"],"metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:06:02.054691Z","iopub.execute_input":"2022-02-27T19:06:02.055711Z","iopub.status.idle":"2022-02-27T19:06:03.359303Z","shell.execute_reply.started":"2022-02-27T19:06:02.055672Z","shell.execute_reply":"2022-02-27T19:06:03.35853Z"},"trusted":true,"id":"asQMZTiLqd_T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rf = RandomForestClassifier(random_state=42)\n","gb = GradientBoostingClassifier(random_state=42)\n","ada = AdaBoostClassifier(random_state=42)\n","lgb = LGBMClassifier(random_state=42)\n","xgb = XGBClassifier(eval_metric=\"mlogloss\",random_state=42)\n","dt = DecisionTreeClassifier(random_state=42)\n","svc = SVC(random_state=42)\n","nb = MultinomialNB()\n","mlp = MLPClassifier(random_state=42)\n","\n","clfs = {\n","    \"Random Forest\": rf,\n","    \"Gradient Boosting\":gb,\n","    \"AdaBoost\": ada,\n","    \"LightGBM\": lgb,\n","    \"XGBoost\": xgb,\n","    \"Decision Tree\":dt,\n","    \"Support Vector Machine\":svc,\n","    \"Naive Bayes\": nb,\n","    \"Multilayer Perceptron\":mlp\n","}\n","\n","def fit_model(clf,x_train,y_train,x_test, y_test):\n","    clf.fit(x_train,y_train)\n","    y_pred = clf.predict(x_test)\n","    accuracy = accuracy_score(y_pred, y_test)\n","    return accuracy\n","\n","accuracys = []\n","\n","for name,clf in tqdm(clfs.items()):\n","    curr_acc = fit_model(clf,X_train_tf,y_train,X_test_tf,y_test)\n","    accuracys.append(curr_acc)"],"metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:06:03.360607Z","iopub.execute_input":"2022-02-27T19:06:03.360875Z","iopub.status.idle":"2022-02-27T19:08:22.074766Z","shell.execute_reply.started":"2022-02-27T19:06:03.36084Z","shell.execute_reply":"2022-02-27T19:08:22.073728Z"},"trusted":true,"id":"eoHaVMoVqd_T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_df = pd.DataFrame({\"Models\":clfs.keys(),\"Accuracy Scores\":accuracys}).sort_values('Accuracy Scores',ascending=False)\n","models_df"],"metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:08:22.078315Z","iopub.execute_input":"2022-02-27T19:08:22.078655Z","iopub.status.idle":"2022-02-27T19:08:22.106555Z","shell.execute_reply.started":"2022-02-27T19:08:22.078612Z","shell.execute_reply":"2022-02-27T19:08:22.105692Z"},"trusted":true,"id":"iaCMHm2rqd_U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Transformers to the rescue\n","It looks like the previous results are unpromising. Let's bring out the Big Guns. HuggingFace acceleate is another amazing library that manages training really well, it doesn't matter which backend you're using (CPU/GPU/TPU). the accelerate library will let you train your model without any code changes. Using TPU's to train your model is as easy as install `torch-xla` and running `notebook_launcher(training_function)`"],"metadata":{"id":"xMYsh2Gaqd_V"}},{"cell_type":"code","source":["def create_dataset(dataframe):\n","    training_df, test_df = train_test_split(\n","        dataframe,\n","        test_size=cfg.test_size,\n","        random_state=cfg.random_seed,\n","        stratify=dataframe.labels.values,\n","    )\n","    train_df, val_df = train_test_split(\n","        training_df,\n","        test_size=cfg.validation_size,\n","        random_state=cfg.random_seed,\n","        stratify=training_df.labels.values,\n","    )\n","\n","    dataset = {\n","        \"train\": Dataset.from_pandas(train_df),\n","        \"validation\": Dataset.from_pandas(val_df),\n","        \"test\": Dataset.from_pandas(test_df),\n","    }\n","\n","    dataset = DatasetDict(dataset)\n","\n","    return dataset"],"metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:08:22.111256Z","iopub.execute_input":"2022-02-27T19:08:22.111783Z","iopub.status.idle":"2022-02-27T19:08:22.123124Z","shell.execute_reply.started":"2022-02-27T19:08:22.111734Z","shell.execute_reply":"2022-02-27T19:08:22.12226Z"},"trusted":true,"id":"LBHtNoxeqd_W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize_dataset():\n","    dataset = create_dataset(preprocess_csv(cfg.data_path))\n","    tokenizer = AutoTokenizer.from_pretrained(cfg.model_checkpoint,use_fast=True)\n","\n","    def tokenize_function(sample):\n","        outputs = tokenizer(\n","            sample[\"Sentence\"],\n","            truncation=True,\n","            padding=\"max_length\",\n","            max_length=cfg.max_length,\n","        )\n","        return outputs\n","\n","    tokenized_datasets = dataset.map(\n","        tokenize_function, batched=True, remove_columns=[\"Sentence\",\"Sentiment\",\"__index_level_0__\"]\n","    )\n","    tokenized_datasets.set_format(\"torch\")\n","\n","    return tokenized_datasets"],"metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:08:22.124242Z","iopub.execute_input":"2022-02-27T19:08:22.124555Z","iopub.status.idle":"2022-02-27T19:08:22.134169Z","shell.execute_reply.started":"2022-02-27T19:08:22.124524Z","shell.execute_reply":"2022-02-27T19:08:22.133439Z"},"trusted":true,"id":"aCWZzoxmqd_Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_dataloaders():\n","    tokenized_datasets = tokenize_dataset()\n","    train_dataloader = DataLoader(\n","        tokenized_datasets[\"train\"], shuffle=True, batch_size=cfg.train_batch_size\n","    )\n","    eval_dataloader = DataLoader(\n","        tokenized_datasets[\"validation\"], shuffle=False, batch_size=cfg.eval_batch_size\n","    )\n","    return train_dataloader, eval_dataloader"],"metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:08:22.135305Z","iopub.execute_input":"2022-02-27T19:08:22.135993Z","iopub.status.idle":"2022-02-27T19:08:22.142716Z","shell.execute_reply.started":"2022-02-27T19:08:22.135957Z","shell.execute_reply":"2022-02-27T19:08:22.142017Z"},"trusted":true,"id":"dDY7AF8qqd_Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def training_function():\n","    accelerator = Accelerator()\n","\n","    set_seed(cfg.random_seed)\n","    tokenized_datasets = tokenize_dataset()\n","    accuracy = load_metric(\"accuracy\")\n","\n","    if accelerator.is_main_process:\n","        datasets.utils.logging.set_verbosity_warning()\n","        transformers.utils.logging.set_verbosity_info()\n","    else:\n","        datasets.utils.logging.set_verbosity_error()\n","        transformers.utils.logging.set_verbosity_error()\n","\n","    train_dataloader, eval_dataloader = create_dataloaders()\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        cfg.model_checkpoint, num_labels=cfg.num_labels\n","    )\n","    optimizer = torch.optim.AdamW(\n","        params=model.parameters(), eps=cfg.adam_epsilon, lr=cfg.lr\n","    )\n","    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n","        model, optimizer, train_dataloader, eval_dataloader\n","    )\n","    lr_scheduler = get_linear_schedule_with_warmup(\n","        optimizer=optimizer,\n","        num_warmup_steps=cfg.num_warmup_steps,\n","        num_training_steps=len(train_dataloader) * cfg.epochs,\n","    )\n","    progress_bar = tqdm(\n","        range(cfg.epochs * len(train_dataloader)),\n","        disable=not accelerator.is_main_process,\n","    )\n","\n","    # Model Training\n","    for epoch in range(cfg.epochs):\n","        model.train()\n","        for step, batch in enumerate(train_dataloader):\n","            outputs = model(**batch)\n","            loss = outputs.loss\n","            accelerator.backward(loss)\n","\n","            optimizer.step()\n","            lr_scheduler.step()\n","            optimizer.zero_grad()\n","            progress_bar.update(1)\n","\n","        model.eval()\n","        all_predictions = []\n","        all_labels = []\n","\n","        for step, batch in enumerate(eval_dataloader):\n","            with torch.no_grad():\n","                outputs = model(**batch)\n","            predictions = outputs.logits.argmax(dim=-1)\n","\n","            # gather predictions and labels from the 8 TPUs\n","            all_predictions.append(accelerator.gather(predictions))\n","            all_labels.append(accelerator.gather(batch[\"labels\"]))\n","\n","        # Concatenate all predictions and labels.\n","        all_predictions = torch.cat(all_predictions)[\n","            : len(tokenized_datasets[\"validation\"])\n","        ]\n","        all_labels = torch.cat(all_labels)[: len(tokenized_datasets[\"validation\"])]\n","\n","        eval_accuracy = accuracy.compute(\n","            predictions=all_predictions, references=all_labels\n","        )\n","\n","        # Use accelerator.print to print only on the main process.\n","        accelerator.print(f\"epoch {epoch}:\", eval_accuracy)"],"metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:08:22.144033Z","iopub.execute_input":"2022-02-27T19:08:22.144497Z","iopub.status.idle":"2022-02-27T19:08:22.166191Z","shell.execute_reply.started":"2022-02-27T19:08:22.144462Z","shell.execute_reply":"2022-02-27T19:08:22.165429Z"},"trusted":true,"id":"ScqTbI3Oqd_Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from accelerate import notebook_launcher\n","notebook_launcher(training_function)"],"metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:08:22.167767Z","iopub.execute_input":"2022-02-27T19:08:22.168334Z","iopub.status.idle":"2022-02-27T19:13:11.84711Z","shell.execute_reply.started":"2022-02-27T19:08:22.168295Z","shell.execute_reply":"2022-02-27T19:13:11.846334Z"},"trusted":true,"id":"-vtZU1M6qd_Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Conclusion"],"metadata":{"id":"xXG9DC7qqd_Z"}},{"cell_type":"code","source":["RoBERTa = {'Models': 'RoBERTa', 'Accuracy Scores': 0.8262}\n","models_df = models_df.append(RoBERTa, ignore_index = True)"],"metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:13:11.851277Z","iopub.execute_input":"2022-02-27T19:13:11.851477Z","iopub.status.idle":"2022-02-27T19:13:11.858641Z","shell.execute_reply.started":"2022-02-27T19:13:11.851452Z","shell.execute_reply":"2022-02-27T19:13:11.85787Z"},"trusted":true,"id":"d7iHK5pIqd_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.rcParams['figure.figsize']=22,10\n","sns.set_style(\"darkgrid\")\n","ax = sns.barplot(x=models_df[\"Models\"], y=models_df[\"Accuracy Scores\"], palette = \"coolwarm\", saturation =1.5)\n","plt.xlabel(\"Classification Models\", fontsize = 20 )\n","plt.ylabel(\"Accuracy\", fontsize = 20)\n","plt.title(\"Accuracy of different Classification Models\", fontsize = 20)\n","plt.xticks(fontsize = 11, horizontalalignment = 'center', rotation = 8)\n","plt.yticks(fontsize = 13)\n","for p in ax.patches:\n","    width, height = p.get_width(), p.get_height()\n","    x, y = p.get_xy()\n","    ax.annotate(f'{height:.2%}', (x + width/2, y + height*1.02), ha='center', fontsize = 'x-large')\n","plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:13:11.86008Z","iopub.execute_input":"2022-02-27T19:13:11.860336Z","iopub.status.idle":"2022-02-27T19:13:12.215619Z","shell.execute_reply.started":"2022-02-27T19:13:11.860301Z","shell.execute_reply":"2022-02-27T19:13:12.214923Z"},"trusted":true,"id":"f4BF4YV_qd_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FOlRlgVQqd_Z"},"execution_count":null,"outputs":[]}]}